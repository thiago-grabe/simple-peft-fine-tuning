{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "Fine-tuning foundation models with minimal resources is crucial for adapting these models to specific tasks, enabling customization without significant computational overhead. One of the most efficient ways to achieve this is through lightweight fine-tuning techniques, which ensure flexibility and lower resource requirements.\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The **Hate Speech Twitter** dataset ([view here](https://huggingface.co/datasets/thefrankhsu/hate_speech_twitter/viewer)) is curated to examine and mitigate hate speech on social media platforms, particularly Twitter. It is divided into two parts: training and testing datasets. Both sets contain tweets labeled and categorized into nine distinct categories related to various forms of hate speech.\n",
    "\n",
    "### Key Features of the Dataset\n",
    "\n",
    "This dataset includes three primary features: \n",
    "\n",
    "- **Tweets**: The actual content of the tweets.\n",
    "- **Labels**: Binary values indicating whether the tweet contains hate speech (1) or not (0).\n",
    "- **Categories**: Nine categories for hate speech classification: behavior, class, disability, ethnicity, gender, physical appearance, race, religion, and sexual orientation.\n",
    "\n",
    "### Dataset Breakdown\n",
    "\n",
    "- **Training Set**: \n",
    "  - Total tweets: 5679\n",
    "  - Hate Speech: 1516 \n",
    "  - Non-Hate Speech: 4163 \n",
    "  - Note: Hate speech is not evenly distributed across categories.\n",
    "\n",
    "- **Testing Set**: \n",
    "  - Total tweets: 1000\n",
    "  - Hate Speech: 500\n",
    "  - Non-Hate Speech: 500\n",
    "  - Note: Hate speech categories are more evenly distributed.\n",
    "\n",
    "### Applications\n",
    "\n",
    "This dataset can be applied to several key tasks, such as:\n",
    "\n",
    "- **Hate Speech Detection**: Developing and training models to identify hate speech on social media.\n",
    "- **Prevalence Analysis**: Studying patterns and trends of hate speech across various categories.\n",
    "- **Categorization Challenges**: Investigating the complexities of accurately classifying hate speech in online platforms.\n",
    "\n",
    "### Model Training Details\n",
    "\n",
    "- **PEFT Technique**: [LoRA (Low-Rank Adaptation)](https://github.com/huggingface/peft)\n",
    "- **Base Model**: GPT-2\n",
    "- **Evaluation Approach**: Transformer trainer\n",
    "- **Fine-Tuning Dataset**: [Hate Speech Twitter](https://huggingface.co/datasets/thefrankhsu/hate_speech_twitter/viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04472fc2-dd83-4803-96e8-7f2888a71947",
   "metadata": {},
   "source": [
    "### Libraries for Deep Learning and Model Training:\n",
    "\n",
    "1. **`import torch`**\n",
    "   - **Purpose**: `torch` is the core library for PyTorch, which is an open-source deep learning framework. It provides tools for tensor operations, building neural networks, and managing training loops.\n",
    "   - **Why Needed**: It is essential for tensor operations, moving models to devices (CPU/GPU), and performing various machine learning operations like backpropagation, gradient calculation, etc.\n",
    "\n",
    "2. **`from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EvalPrediction, DataCollatorWithPadding`**\n",
    "   - **Purpose**: This set of imports comes from the **Hugging Face Transformers library**, which provides pre-trained models and tools for NLP (Natural Language Processing).\n",
    "     - **`AutoTokenizer`**: Automatically loads a tokenizer for a pre-trained model. Tokenizers convert raw text into token IDs that a model can understand.\n",
    "     - **`AutoModelForSequenceClassification`**: Automatically loads a pre-trained model for sequence classification tasks, such as sentiment analysis, text classification, etc.\n",
    "     - **`TrainingArguments`**: Defines the configuration and hyperparameters for model training, such as learning rate, batch size, and number of epochs.\n",
    "     - **`Trainer`**: A class that simplifies the process of training and evaluating models. It handles training loops, gradient computation, logging, and evaluation.\n",
    "     - **`EvalPrediction`**: Used to define the format for predictions and labels during evaluation (often used in the `Trainer`'s evaluation step).\n",
    "     - **`DataCollatorWithPadding`**: A collator that dynamically pads input sequences to the same length during batching. This ensures that the model receives input data of consistent dimensions.\n",
    "   - **Why Needed**: These components are crucial for working with pre-trained models, tokenizing text, setting training configurations, and performing the training and evaluation steps. They streamline many common tasks in NLP tasks.\n",
    "\n",
    "3. **`from datasets import Dataset`**\n",
    "   - **Purpose**: `Dataset` is a class from the Hugging Face **Datasets library**, which is used to handle and manipulate datasets in a format compatible with the Hugging Face ecosystem.\n",
    "   - **Why Needed**: This is needed for creating and managing datasets for training and evaluation. It supports efficient loading and processing of large datasets.\n",
    "\n",
    "### Libraries for Machine Learning and Evaluation:\n",
    "\n",
    "4. **`from sklearn.model_selection import train_test_split`**\n",
    "   - **Purpose**: `train_test_split` is a function from **Scikit-learn**, a machine learning library, that splits data into training and testing subsets.\n",
    "   - **Why Needed**: This is crucial for dividing your dataset into training and validation sets, allowing you to evaluate the model's performance on unseen data during training.\n",
    "\n",
    "5. **`from sklearn.metrics import accuracy_score, precision_recall_fscore_support`**\n",
    "   - **Purpose**: These are functions from **Scikit-learn** used to evaluate the model's performance.\n",
    "     - **`accuracy_score`**: Computes the accuracy of the model, which is the proportion of correctly classified instances.\n",
    "     - **`precision_recall_fscore_support`**: Computes additional classification metrics such as precision, recall, and F1 score, which are useful for understanding model performance beyond simple accuracy.\n",
    "   - **Why Needed**: These are essential for assessing the performance of the model on the validation dataset, especially when you want to evaluate not just accuracy but also precision and recall (which are more informative in cases of class imbalance).\n",
    "\n",
    "### Libraries for Parameter-Efficient Fine-Tuning (PEFT):\n",
    "\n",
    "6. **`from peft import LoraConfig, PeftModelForSequenceClassification, TaskType, AutoPeftModelForSequenceClassification`**\n",
    "   - **Purpose**: These imports come from a **custom PEFT (Parameter-Efficient Fine-Tuning)** module, which allows fine-tuning large pre-trained models more efficiently by only updating a small number of parameters.\n",
    "     - **`LoraConfig`**: Contains the configuration settings for the **LoRA (Low-Rank Adaptation)** technique, which is a PEFT method. It helps reduce the number of parameters that need to be updated during fine-tuning.\n",
    "     - **`PeftModelForSequenceClassification`**: Wraps a pre-trained model and applies PEFT (specifically LoRA) to allow fine-tuning with fewer parameters.\n",
    "     - **`TaskType`**: Specifies the task type (e.g., sequence classification, token classification, etc.) for the PEFT model.\n",
    "     - **`AutoPeftModelForSequenceClassification`**: Automatically loads a pre-trained model and wraps it for PEFT, making it easier to work with models fine-tuned using LoRA for sequence classification tasks.\n",
    "   - **Why Needed**: PEFT is used here to fine-tune models with fewer parameters, making it possible to fine-tune large models without the need for massive computational resources. These imports are used for loading, configuring, and fine-tuning the model efficiently.\n",
    "\n",
    "### Summary:\n",
    "- **PyTorch** (`torch`) is used for model operations and training loops.\n",
    "- **Hugging Face Transformers** handles the model loading, tokenization, and training infrastructure for NLP tasks.\n",
    "- **Scikit-learn** is used for dataset splitting and evaluating model performance.\n",
    "- **PEFT** (Parameter-Efficient Fine-Tuning) tools are used to fine-tune the model in a resource-efficient manner.\n",
    "\n",
    "Together, these libraries provide all the necessary tools to manage datasets, train models, evaluate performance, and fine-tune large language models efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d49a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, PeftModelForSequenceClassification, TaskType, AutoPeftModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b84e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = pd.read_csv('./dataset/training_set.csv', usecols=['tweet', 'label']).dropna()\n",
    "ds_test  = pd.read_csv('./dataset/testing_set.csv', usecols=['tweet', 'label']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5888ef22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>krazy i dont always get drunk and pass out but...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white kids favorite activities calling people ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>maam did you clear that tweet with the   caref...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wth is that playing missy  i mean seriously rt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he promised to stand with the muzzies so</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  krazy i dont always get drunk and pass out but...      0\n",
       "1  white kids favorite activities calling people ...      1\n",
       "2  maam did you clear that tweet with the   caref...      0\n",
       "3  wth is that playing missy  i mean seriously rt...      0\n",
       "4           he promised to stand with the muzzies so      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a0b338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sad to hear the announcers say that \"it may ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spazzies aren't welcome around here</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gay people need to die</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the big screen is being fitted right now   #eu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why is it that African people smell weird? Do ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label\n",
       "0  sad to hear the announcers say that \"it may ha...      0\n",
       "1                Spazzies aren't welcome around here      1\n",
       "2                             gay people need to die      1\n",
       "3  the big screen is being fitted right now   #eu...      0\n",
       "4  Why is it that African people smell weird? Do ...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b69455f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(ds_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6814a66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4542 entries, 1810 to 4396\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   4542 non-null   object\n",
      " 1   label   4542 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 106.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b224cbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1136 entries, 4868 to 4901\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tweet   1136 non-null   object\n",
      " 1   label   1136 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 26.6+ KB\n"
     ]
    }
   ],
   "source": [
    "val_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a389fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate2label = {1:\"hate speeach\", 0:\"Neutral\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56b10f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframes into Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01094862",
   "metadata": {},
   "source": [
    "### Initializing the Tokenizer\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "```\n",
    "- **Tokenizer Creation**: Utilizes the `gpt2` tokenizer from the Hugging Face Transformers library.  \n",
    "- **Setting the Pad Token**: As GPT-2 doesn't use a padding token by default, the `pad_token` is set to the end-of-sequence (`eos`) token to maintain consistency in padding.\n",
    "\n",
    "### Defining the `tokenize_and_encode` Function\n",
    "```python\n",
    "def tokenize_and_encode(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tweet'],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    tokenized_inputs['labels'] = examples['label']\n",
    "    return tokenized_inputs\n",
    "```\n",
    "- **Function Purpose**: Processes a batch of examples and returns token IDs that are compatible with the model for training.  \n",
    "- **Parameters**:\n",
    "  - `examples['tweet']`: The tweet text to be tokenized.\n",
    "  - `padding=\"max_length\"`: Ensures that all sequences are padded to a uniform maximum length.\n",
    "  - `truncation=True`: Ensures sequences longer than the `max_length` are truncated to avoid overflow.\n",
    "  - `max_length=512`: Specifies the maximum length for each tokenized sequence.\n",
    "- **Assigning Labels**: The labels from `examples['label']` are added to the `tokenized_inputs` dictionary under the key `'labels'`, making both inputs and labels accessible for the model in one dataset.\n",
    "\n",
    "### Applying the Function to the Datasets\n",
    "```python\n",
    "train_dataset = train_dataset.map(tokenize_and_encode, batched=True)\n",
    "val_dataset   = val_dataset.map(tokenize_and_encode, batched=True)\n",
    "```\n",
    "- **Mapping Over Datasets**: The `.map()` function applies the `tokenize_and_encode` method to each batch of examples in both the training and validation datasets.\n",
    "- **Why `batched=True`?**: Using `batched=True` in Hugging Face Datasets allows for processing multiple examples at once, improving efficiency over processing them individually.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**:  \n",
    "- **Purpose**: Prepares textual data and labels for a model that requires GPT-2 tokenized input.  \n",
    "- **Key Steps**: Initialize the tokenizer, define a function to tokenize and encode the text while attaching labels, and then apply this function to the training and validation datasets.  \n",
    "- **Outcome**: The `train_dataset` and `val_dataset` are now formatted appropriately for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0598e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c416a037359406baadadf87088eab41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee774a87b8514f9db7badf900be6adfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3674eb1a7cc490796163ddcd434c9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3f0aa7a5f945f285d1c98f70f4ed03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096a23712d6f430da1c51e66350b2a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390a77dce3414c5a851277e4531aca8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26da8700912b4dddae69e435420220cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1136 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize and convert\n",
    "def tokenize_and_encode(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tweet'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokenized_inputs['labels'] = examples['label']\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_encode, batched=True)\n",
    "val_dataset   = val_dataset.map(tokenize_and_encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cabf3b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a message for all of humanity  charlie chaplin  automatic\n"
     ]
    }
   ],
   "source": [
    "random_idx = random.randint(0, len(train_dataset) - 1)\n",
    "print(train_dataset[random_idx][\"tweet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e274f4b",
   "metadata": {},
   "source": [
    "### Clearing CUDA Cache\n",
    "```python\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "- **Purpose**: Clears unused memory from the GPU cache, freeing up resources.  \n",
    "- **Why It Matters**: Helps mitigate memory fragmentation issues and reduces the risk of running into out-of-memory errors during model training.\n",
    "\n",
    "### Initializing the Model\n",
    "```python\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "```\n",
    "- **Using GPT-2 for Classification**: Loads GPT-2 as the base model and configures it for sequence classification with two output labels.  \n",
    "- **Pad Token ID**: Sets the model’s `config.pad_token_id` to match the tokenizer's padding token to ensure consistent padding behavior across model and tokenizer.\n",
    "\n",
    "### Defining the Data Collator\n",
    "```python\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "```\n",
    "- **Automatic Padding**: Ensures sequences in each batch are padded to the same length, regardless of their original length.  \n",
    "- **Tokenizer Integration**: Leverages the GPT-2 tokenizer to handle padding correctly.\n",
    "\n",
    "### Compute Metrics Function\n",
    "```python\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "```\n",
    "- **Purpose**: Computes key evaluation metrics like accuracy, precision, recall, and F1 score from the model’s raw predictions.  \n",
    "- **Key Steps**:  \n",
    "  - `np.argmax(...)` converts logits into predicted class indices.  \n",
    "  - `precision_recall_fscore_support(...)` calculates the precision, recall, and F1 scores.  \n",
    "  - `accuracy_score(...)` calculates the overall accuracy of the predictions.\n",
    "\n",
    "### Training Arguments\n",
    "```python\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_normal_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2.5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_normal_model',\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=150,\n",
    "    warmup_ratio=0.1,\n",
    "    eval_accumulation_steps=50\n",
    ")\n",
    "```\n",
    "- **Output Directory**: Stores model checkpoints and logs at `./results_normal_model`.  \n",
    "- **Evaluation Strategy**: Evaluates the model at the end of each epoch (`evaluation_strategy=\"epoch\"`).  \n",
    "- **Learning Rate**: Sets a learning rate of `2.5e-5`, commonly used for fine-tuning large models.  \n",
    "- **Batch Sizes**: Specifies batch sizes of 8 for both training and evaluation.  \n",
    "- **Epochs**: Trains the model for 1 epoch. More epochs can be added if necessary.  \n",
    "- **Weight Decay**: Adds regularization to prevent overfitting.  \n",
    "- **Logging & Saving**: Configures logging frequency (`logging_steps=150`) and saves the model at the end of each epoch.  \n",
    "- **Load Best Model**: Ensures the best model (based on validation performance) is selected at the end of training.  \n",
    "- **Warmup Ratio**: Allocates 10% of the total training steps for learning rate warmup (`warmup_ratio=0.1`).  \n",
    "- **Evaluation Accumulation**: Sets `eval_accumulation_steps=50` to accumulate predictions in small batches for memory efficiency.\n",
    "\n",
    "### Initializing the Trainer\n",
    "```python\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "```\n",
    "- **Components**:  \n",
    "  - **Model**: The GPT-2 model configured for classification.  \n",
    "  - **Training Arguments**: The configuration set defined earlier.  \n",
    "  - **Datasets**: Uses both the training and validation datasets.  \n",
    "  - **Metrics**: Includes the `compute_metrics` function to evaluate performance.  \n",
    "  - **Tokenizer & Data Collator**: Handles tokenization and padding for the data.\n",
    "\n",
    "### Training\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "- **Start Training**: Begins the fine-tuning process, logging training loss, evaluating after each epoch, and saving model checkpoints.\n",
    "\n",
    "### Evaluation\n",
    "```python\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", evaluation_results)\n",
    "```\n",
    "- **Final Evaluation**: Computes the evaluation metrics for the validation dataset using the model trained with the best performance.  \n",
    "- **Results**: Prints the metrics, including accuracy, precision, recall, and F1 score.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**  \n",
    "This code shows how to configure and fine-tune a GPT-2 model for binary classification using Hugging Face’s Trainer. The steps include:\n",
    "- **Model Setup** for classification with GPT-2.  \n",
    "- **Data Preparation** with a padding collator.  \n",
    "- **Metric Calculation** for evaluation (accuracy, precision, recall, F1).  \n",
    "- **Training Strategy** with logging, evaluation, and checkpointing.  \n",
    "- **Evaluation** to assess model performance on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2a491761d846c2b135177e0b788e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='568' max='568' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [568/568 08:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.290100</td>\n",
       "      <td>0.269737</td>\n",
       "      <td>0.923415</td>\n",
       "      <td>0.923783</td>\n",
       "      <td>0.924291</td>\n",
       "      <td>0.923415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results_normal_model/checkpoint-568 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [142/142 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.26973676681518555, 'eval_accuracy': 0.9234154929577465, 'eval_f1': 0.923782679107111, 'eval_precision': 0.9242905024393862, 'eval_recall': 0.9234154929577465, 'eval_runtime': 43.4052, 'eval_samples_per_second': 26.172, 'eval_steps_per_second': 3.271, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# Compute metrics function\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    return {\"accuracy\": accuracy_score(p.label_ids, preds), \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_normal_model\",  # Directory to save model checkpoints and logs\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    learning_rate=2.5e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=8,  # Batch size for training on each device (GPU/CPU)\n",
    "    per_device_eval_batch_size=8,  # Batch size for evaluation on each device\n",
    "    num_train_epochs=1,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Weight decay (regularization) to prevent overfitting\n",
    "    logging_dir='./logs_normal_model',  # Directory to save logs for tracking\n",
    "    save_strategy=\"epoch\",  # Save model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model based on evaluation metrics after training\n",
    "    logging_steps=150,  # Number of steps between each logging event\n",
    "    warmup_ratio=0.1,  # Fraction of total steps for learning rate warmup (gradual increase)\n",
    "    eval_accumulation_steps=50  # Number of steps to accumulate predictions for evaluation (to save memory)\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with compute_metrics\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c173886",
   "metadata": {},
   "source": [
    "### Defining the LoRA Configuration\n",
    "```python\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "```\n",
    "- **PEFT (Parameter-Efficient Fine-Tuning)**: A method designed to fine-tune large language models using fewer trainable parameters, making the process more resource-efficient.  \n",
    "- **LoRA (Low-Rank Adaptation)**: A specific PEFT technique that introduces low-rank adaptation matrices into certain layers of a pre-trained model, reducing the number of parameters that need to be updated.  \n",
    "- **Key Configuration Parameters**:  \n",
    "  - **`task_type=TaskType.SEQ_CLS`**: Specifies that this task is for sequence classification.  \n",
    "  - **`inference_mode=False`**: Indicates the model is in training mode, allowing updates to the LoRA parameters.  \n",
    "  - **`r=4`**: Sets the rank of the LoRA adaptation matrices, with a smaller rank reducing the number of parameters.  \n",
    "  - **`lora_alpha=16`**: Scaling factor for LoRA, controlling the magnitude of updates.  \n",
    "  - **`lora_dropout=0.1`**: Applies dropout to the LoRA layers to reduce overfitting.\n",
    "\n",
    "### Loading the Pre-trained GPT-2 Model\n",
    "```python\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "```\n",
    "- **GPT-2 for Classification**: Loads GPT-2 as a base model and adapts it for sequence classification with `num_labels=2`.  \n",
    "- **Padding Token**: Since GPT-2 doesn’t have a padding token, we set the `pad_token_id` to the end-of-sequence (`eos_token_id`) token.\n",
    "\n",
    "### Wrapping the Model with LoRA (PEFT)\n",
    "```python\n",
    "peft_model = PeftModelForSequenceClassification(model, peft_config)\n",
    "```\n",
    "- **PeftModelForSequenceClassification**: Wraps the GPT-2 classification model with additional LoRA layers, which are the only parameters being fine-tuned.  \n",
    "- **Goal**: This approach ensures that the majority of the model's parameters remain frozen, optimizing memory and computational efficiency during fine-tuning.\n",
    "\n",
    "### Checking Trainable Parameters\n",
    "```python\n",
    "peft_model.print_trainable_parameters()\n",
    "```\n",
    "- **Function**: Prints the number of trainable parameters in the model, confirming that only the LoRA parameters and possibly the classification head are being updated.  \n",
    "- **Purpose**: Verifies that the majority of the model’s parameters are frozen, which reduces the computational and memory overhead of fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**  \n",
    "This code demonstrates how to apply **LoRA** (Low-Rank Adaptation), a parameter-efficient fine-tuning approach, to a **GPT-2** model for sequence classification. By defining a **LoRA configuration** and wrapping the model with **`PeftModelForSequenceClassification`**, fine-tuning is performed on a smaller subset of parameters, making the process faster and more resource-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 150,528 || all params: 124,590,336 || trainable%: 0.1208183594592762\n"
     ]
    }
   ],
   "source": [
    "# PEFT model configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Specifies the task type (sequence classification)\n",
    "    inference_mode=False,  # Set to False for training mode (enables parameter updates)\n",
    "    r=4,  # Rank of the low-rank adaptation matrices (controls how many parameters are adapted)\n",
    "    lora_alpha=16,  # Scaling factor for the LoRA updates (higher values mean larger updates)\n",
    "    lora_dropout=0.1  # Dropout rate applied to LoRA layers to reduce overfitting\n",
    ")\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)  \n",
    "\n",
    "# Loads GPT-2 and adapts it for sequence classification with two output labels (binary classification task)\n",
    "model.config.pad_token_id = model.config.eos_token_id  # Sets pad_token_id to eos_token_id as GPT-2 does not have a dedicated padding token\n",
    "\n",
    "# Wrap the model with PEFT (Parameter Efficient Fine-Tuning) using LoRA\n",
    "peft_model = PeftModelForSequenceClassification(model, peft_config)  # Applies the LoRA configuration to the GPT-2 model\n",
    "\n",
    "# Print the number of trainable parameters in the PEFT model\n",
    "peft_model.print_trainable_parameters()  # Displays how many parameters are trainable, confirming the effectiveness of PEFT (only LoRA parameters are updated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='568' max='568' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [568/568 07:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.556955</td>\n",
       "      <td>0.742958</td>\n",
       "      <td>0.644517</td>\n",
       "      <td>0.717799</td>\n",
       "      <td>0.742958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results_perf_model/checkpoint-568 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [142/142 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.5569546222686768, 'eval_accuracy': 0.7429577464788732, 'eval_f1': 0.6445174350830246, 'eval_precision': 0.7177987712370479, 'eval_recall': 0.7429577464788732, 'eval_runtime': 44.3861, 'eval_samples_per_second': 25.594, 'eval_steps_per_second': 3.199, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_perf_model\",  # Directory where model checkpoints and logs will be saved\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    learning_rate=2.5e-5,  # Set the learning rate for the optimizer\n",
    "    per_device_train_batch_size=8,  # Batch size used for training on each device (GPU/CPU)\n",
    "    per_device_eval_batch_size=8,  # Batch size used for evaluation on each device\n",
    "    num_train_epochs=1,  # Number of training epochs (how many times the model will see the full dataset)\n",
    "    weight_decay=0.01,  # Weight decay applied to the optimizer to prevent overfitting\n",
    "    logging_dir='./logs_perf_model',  # Directory where logs (training progress, metrics) will be saved\n",
    "    save_strategy=\"epoch\",  # Save model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model based on evaluation metrics after training\n",
    "    logging_steps=150,  # Number of steps between logging events (e.g., showing training progress)\n",
    "    warmup_ratio=0.1,  # Fraction of total training steps used for learning rate warmup\n",
    "    eval_accumulation_steps=50  # Number of steps to accumulate predictions before running evaluation (to save memory)\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with compute_metrics\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained('model/peft_model_tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "This code snippet demonstrates how to load a fine-tuned model, set up a **Trainer** for evaluation, and then evaluate its performance. Let's break it down step by step:\n",
    "\n",
    "### Loading the Fine-Tuned Model\n",
    "```python\n",
    "inference_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"model/peft_model\",\n",
    "    num_labels=2\n",
    ")\n",
    "inference_model.config.pad_token_id = inference_model.config.eos_token_id\n",
    "```\n",
    "- **`AutoPeftModelForSequenceClassification.from_pretrained(\"model/peft_model\", num_labels=2)`**:  \n",
    "  - This line loads a pre-trained model from the directory `\"model/peft_model\"`. The model is configured for sequence classification with `num_labels=2`, indicating it is a binary classification task (e.g., hate speech vs. non-hate speech).\n",
    "  - **PEFT**: The model is based on **Parameter-Efficient Fine-Tuning (PEFT)**, which allows fine-tuning large models with fewer trainable parameters.\n",
    "  \n",
    "- **`inference_model.config.pad_token_id = inference_model.config.eos_token_id`**:  \n",
    "  - Since the model might not have a dedicated padding token, the padding token (`pad_token_id`) is set to the **end-of-sequence (`eos_token_id`)** token, ensuring consistency in padding for sequences.\n",
    "\n",
    "### Setting Up the Trainer\n",
    "```python\n",
    "trainer = Trainer(\n",
    "    model=inference_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "```\n",
    "- **`Trainer`**: The Hugging Face `Trainer` class is used to simplify training and evaluation of models. In this case, it’s being set up for evaluation.\n",
    "  \n",
    "- **Model**:  \n",
    "  - The **inference model** (`inference_model`), which is the fine-tuned PEFT model loaded earlier, is provided to the `Trainer`.\n",
    "  \n",
    "- **Training Arguments** (`training_args`):  \n",
    "  - The `training_args` object contains configuration for training and evaluation (e.g., batch size, learning rate, evaluation strategy, etc.). These are set up separately and passed into the `Trainer`.\n",
    "  \n",
    "- **Evaluation Dataset** (`eval_dataset=val_dataset`):  \n",
    "  - The `Trainer` will evaluate the model on the **validation dataset** (`val_dataset`).\n",
    "  \n",
    "- **Metrics** (`compute_metrics=compute_metrics`):  \n",
    "  - The `compute_metrics` function, which is defined elsewhere, computes evaluation metrics (e.g., accuracy, precision, recall) based on the model's predictions and ground truth labels.\n",
    "  \n",
    "- **Tokenizer** (`tokenizer=tokenizer`):  \n",
    "  - The tokenizer is provided to handle tokenization of input text for evaluation, ensuring the model gets the correctly preprocessed input.\n",
    "  \n",
    "- **Data Collator** (`data_collator=data_collator`):  \n",
    "  - The data collator ensures that sequences are padded correctly, allowing the model to handle inputs of varying lengths in the same batch.\n",
    "\n",
    "### Evaluating the Model\n",
    "```python\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", evaluation_results)\n",
    "```\n",
    "- **`trainer.evaluate()`**:  \n",
    "  - This line triggers the evaluation of the model on the validation dataset. It computes the evaluation metrics defined in `compute_metrics` (e.g., accuracy, F1 score) based on the predictions made by the model on the validation data.\n",
    "  \n",
    "- **`evaluation_results`**:  \n",
    "  - The evaluation results are returned as a dictionary containing the computed metrics (e.g., accuracy, precision, recall, etc.).\n",
    "  \n",
    "- **`print(\"Evaluation Results:\", evaluation_results)`**:  \n",
    "  - Finally, the evaluation results are printed to the console so you can see how well the model performs on the validation set.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "In summary, this code loads a fine-tuned **PEFT model** for sequence classification, sets up the **Trainer** for evaluation, and then evaluates the model on a validation dataset. The results are printed, allowing you to assess the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='142' max='142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [142/142 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.6069970726966858, 'eval_accuracy': 0.6566901408450704, 'eval_f1': 0.630114732355021, 'eval_precision': 0.6124590942547943, 'eval_recall': 0.6566901408450704, 'eval_runtime': 45.8782, 'eval_samples_per_second': 24.761, 'eval_steps_per_second': 3.095}\n"
     ]
    }
   ],
   "source": [
    "inference_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"model/peft_model\",\n",
    "    num_labels=2\n",
    ")\n",
    "inference_model.config.pad_token_id = inference_model.config.eos_token_id\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=inference_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Function created to predict/classify each sentence\n",
    "    \n",
    "    Args:\n",
    "        sentence: Sentence to be classified.\n",
    "        \n",
    "    Return:\n",
    "        predicted_class_id: class ID predicted by the model\n",
    "        predicted_label: actual label predicted\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inference_model.to(device)\n",
    "\n",
    "    # Prepare the input text\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = inference_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    predicted_class_id = probabilities.argmax().item()\n",
    "    predicted_label = hate2label[predicted_class_id]\n",
    "\n",
    "    return predicted_class_id, predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'I love you'\n",
      "Predicted Class: 0 - Predicted Label: Neutral\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I love you\"\n",
    "predicted_class, predicted_label = predict(prompt)\n",
    "print(f\"Prompt: '{prompt}'\\nPredicted Class: {predicted_class} - Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'maam did you clear that tweet with the careful they may brand you race traitor for the nerve of thinking'\n",
      "Predicted Class: 0 - Predicted Label: Neutral\n"
     ]
    }
   ],
   "source": [
    "prompt = \"maam did you clear that tweet with the careful they may brand you race traitor for the nerve of thinking\"\n",
    "predicted_class, predicted_label = predict(prompt)\n",
    "print(f\"Prompt: '{prompt}'\\nPredicted Class: {predicted_class} - Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "481f7ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'cant take u coons nowhere amp i mean nowhere'\n",
      "Predicted Class: 0 - Predicted Label: Neutral\n"
     ]
    }
   ],
   "source": [
    "prompt = \"cant take u coons nowhere amp i mean nowhere\"\n",
    "predicted_class, predicted_label = predict(prompt)\n",
    "print(f\"Prompt: '{prompt}'\\nPredicted Class: {predicted_class} - Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8756a16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '0'\n",
      "Predicted Class: 0 - Predicted Label: Neutral\n"
     ]
    }
   ],
   "source": [
    "prompt = \"0\"\n",
    "predicted_class, predicted_label = predict(prompt)\n",
    "print(f\"Prompt: '{prompt}'\\nPredicted Class: {predicted_class} - Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "776e3c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: grand opening   top class  oriental massage  perfect\n",
      "Label:  Neutral\n",
      "Prediction: Neutral\n",
      "\n",
      "Prompt: we dem birds\n",
      "Label:  Neutral\n",
      "Prediction: Neutral\n",
      "\n",
      "Prompt: ill break a bitch down like the pussy she is\n",
      "Label:  hate speeach\n",
      "Prediction: Neutral\n",
      "\n",
      "Prompt: parodies of jihadi songs  more effective than asinine muslims should condemn  humorisaweapon\n",
      "Label:  Neutral\n",
      "Prediction: hate speeach\n",
      "\n",
      "Prompt: i agree my fellow cracker\n",
      "Label:  hate speeach\n",
      "Prediction: hate speeach\n",
      "\n",
      "Prompt: nah electric fairy tale seems cool but id rather buy some drugs with that money\n",
      "Label:  Neutral\n",
      "Prediction: Neutral\n",
      "\n",
      "Prompt: poor people are great.... at being everywhere! It's like the plague!\n",
      "Label:  hate speeach\n",
      "Prediction: hate speeach\n",
      "\n",
      "Prompt: lets cripple the vanity industry how actually start liking who we are and how we look\n",
      "Label:  Neutral\n",
      "Prediction: Neutral\n",
      "\n",
      "Prompt: gamboa hoes will be hoes\n",
      "Label:  hate speeach\n",
      "Prediction: Neutral\n",
      "\n",
      "Prompt: im sure  would add this helpful parenthetical explanation if these were redneck peckerwood teabaggers\n",
      "Label:  hate speeach\n",
      "Prediction: Neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _id in [np.random.randint(low=0,high=val_df.shape[0]) for i in range(10)]:\n",
    "    prompt = val_df.iloc[_id]\n",
    "    \n",
    "    print(f\"Prompt: {prompt['tweet'][:150]}\")\n",
    "    \n",
    "    actual_label_id = prompt['label']\n",
    "    actual_label = hate2label[actual_label_id]\n",
    "    print(f'Label:  {actual_label}')\n",
    "    \n",
    "    inputs = tokenizer(prompt['tweet'], return_tensors=\"pt\").to(inference_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = inference_model(**inputs).logits\n",
    "        \n",
    "    predictions = torch.argmax(logits, dim=1).item()\n",
    "    predicted_label = hate2label[predictions]\n",
    "    print(f'Prediction: {predicted_label}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa0161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
